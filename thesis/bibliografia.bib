@book{Sutton,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@book{Szepesvari,
 author = {Szepesvari, Csaba},
 title = {Algorithms for Reinforcement Learning},
 year = {2010},
 isbn = {1608454924, 9781608454921},
 publisher = {Morgan and Claypool Publishers},
} 

@article{TD,
 author = {Sutton, Richard S.},
 title = {Learning to predict by the method of temporal differences},
 year = {1988},
 journal = {Machine Learning 3},
 pages = {9--44},
}

@article{Q-learning,
 author = {Watkins, Christopher J.C.H. and Dayan, Peter},
 title = {Q-Learning},
 year = {1992},
 journal = {Machine Learning 8},
 pages = {279--292},
}

@article{SARSA,
 author = {J. Gordon, Geoffrey},
 year = {1996},
 title = {Chattering in SARSA(lambda) - A CMU Learning Lab Internal Report},
}

@article{MCTS,
 author = {Chaslot, Guillaume and Bakkes, Sander  and Szita, Istvan and Spronck, Pieter},
 title = {Monte-Carlo Tree Search: A New Framework for Game AI},
 year = {2008},
}

@Book{bellman1957,
  author =       "Bellman, Richard",
  title =        "Dynamic Programming",
  publisher =    "Princeton University Press",
  year =         "1957",
  address =   "Princeton, NJ, USA",
  edition =   "1",
  url = "http://books.google.com/books?id=fyVtp3EMxasC&pg=PR5&dq=dynamic+programming+richard+e+bellman&client=firefox-a#v=onepage&q=dynamic%20programming%20richard%20e%20bellman&f=false",
  bib2html_rescat = "General RL",
}

@article{bellman1954,
author = "Bellman, Richard",
fjournal = "Bulletin of the American Mathematical Society",
journal = "Bull. Amer. Math. Soc.",
month = "11",
number = "6",
pages = "503--515",
publisher = "American Mathematical Society",
title = "The theory of dynamic programming",
url = "https://projecteuclid.org:443/euclid.bams/1183519147",
volume = "60",
year = "1954",
}

@book{dlbook,
    title={Deep Learning},
    author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
    publisher={MIT Press},
    year={2016},
}

@article{alphago2016,
author = {Silver, David and Huang, Aja and Maddison, Christopher and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
year = {2016},
month = {01},
pages = {484-489},
title = {Mastering the game of Go with deep neural networks and tree search},
volume = {529},
booktitle = {Nature}
}

@article{alphagozero,
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
year = {2017},
month = {10},
pages = {354-359},
title = {Mastering the game of Go without human knowledge},
volume = {550},
booktitle = {Nature}
}
@misc{openai-compute,
    author    = "OpenAI",
    title     = "AI and Compute",
    url       = "https://blog.openai.com/ai-and-compute/",
}

@misc{dota2,
 author = "OpenAI",
 title = "Dota 2",
 url = "https://blog.openai.com/dota-2/",
}

@ARTICLE{nmt,
   author = {{Bahdanau}, D. and {Cho}, K. and {Bengio}, Y.},
    title = "{Neural Machine Translation by Jointly Learning to Align and Translate}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1409.0473},
 primaryClass = "cs.CL",
 keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
     year = 2014,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.0473B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{dqn,
   author = {{Mnih}, V. and {Kavukcuoglu}, K. and {Silver}, D. and {Graves}, A. and 
	{Antonoglou}, I. and {Wierstra}, D. and {Riedmiller}, M.},
    title = "{Playing Atari with Deep Reinforcement Learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1312.5602},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2013,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1312.5602M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{resnet,
   author = {{He}, K. and {Zhang}, X. and {Ren}, S. and {Sun}, J.},
    title = "{Deep Residual Learning for Image Recognition}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1512.03385},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
     year = 2015,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151203385H},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{resnext,
   author = {{Xie}, S. and {Girshick}, R. and {Doll{\'a}r}, P. and {Tu}, Z. and 
	{He}, K.},
    title = "{Aggregated Residual Transformations for Deep Neural Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1611.05431},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
     year = 2016,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161105431X},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{sdc1,
   author = {{Bojarski}, M. and {Del Testa}, D. and {Dworakowski}, D. and 
	{Firner}, B. and {Flepp}, B. and {Goyal}, P. and {Jackel}, L.~D. and 
	{Monfort}, M. and {Muller}, U. and {Zhang}, J. and {Zhang}, X. and 
	{Zhao}, J. and {Zieba}, K.},
    title = "{End to End Learning for Self-Driving Cars}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1604.07316},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2016,
    month = apr,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160407316B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{sdc2,
   author = {{Huval}, B. and {Wang}, T. and {Tandon}, S. and {Kiske}, J. and 
	{Song}, W. and {Pazhayampallil}, J. and {Andriluka}, M. and 
	{Rajpurkar}, P. and {Migimatsu}, T. and {Cheng-Yue}, R. and 
	{Mujica}, F. and {Coates}, A. and {Ng}, A.~Y.},
    title = "{An Empirical Evaluation of Deep Learning on Highway Driving}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1504.01716},
 primaryClass = "cs.RO",
 keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
     year = 2015,
    month = apr,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150401716H},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{relu, 
author={K. Jarrett and K. Kavukcuoglu and M. Ranzato and Y. LeCun}, 
booktitle={2009 IEEE 12th International Conference on Computer Vision}, 
title={What is the best multi-stage architecture for object recognition?}, 
year={2009}, 
volume={}, 
number={}, 
pages={2146-2153}, 
keywords={feature extraction;object recognition;unsupervised learning;Caltech-101;NORB dataset;feature extraction;feature pooling layer;feature rectification;filter bank;local contrast normalization;multistage architecture;nonlinear transformation;object recognition;supervised learning;unprocessed MNIST dataset;unsupervised learning;Brain modeling;Error analysis;Feature extraction;Filter bank;Gabor filters;Histograms;Image edge detection;Learning systems;Object recognition;Refining}, 
doi={10.1109/ICCV.2009.5459469}, 
ISSN={1550-5499}, 
month={Sept},}

@ARTICLE{activation-fn,
   author = {{Ramachandran}, P. and {Zoph}, B. and {Le}, Q.~V.},
    title = "{Searching for Activation Functions}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1710.05941},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
     year = 2017,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171005941R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{sgd,
   author = {{Bottou}, L. and {Curtis}, F.~E. and {Nocedal}, J.},
    title = "{Optimization Methods for Large-Scale Machine Learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1606.04838},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning, Mathematics - Optimization and Control},
     year = 2016,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160604838B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{mini-batch-sgd,
 author = {Li, Mu and Zhang, Tong and Chen, Yuqiang and Smola, Alexander J.},
 title = {Efficient Mini-batch Training for Stochastic Optimization},
 booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '14},
 year = {2014},
 isbn = {978-1-4503-2956-9},
 location = {New York, New York, USA},
 pages = {661--670},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2623330.2623612},
 doi = {10.1145/2623330.2623612},
 acmid = {2623612},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {big data, distributed computing, machine learning, minibatch, stochastic gradient descent},
}

@article{backprop,
author = {Lecun, Yann},
year = {2001},
month = {08},
pages = {},
title = {A Theoretical Framework for Back-Propagation}
}

@ARTICLE{adam,
   author = {{Kingma}, D.~P. and {Ba}, J.},
    title = "{Adam: A Method for Stochastic Optimization}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1412.6980},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2014,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.6980K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{rmsprop,
    author = {Tieleman, T. and Hinton, G.},
    citeulike-article-id = {13833942},
    citeulike-linkout-0 = {http://www.cs.toronto.edu/\~{}tijmen/csc321/slides/lecture\_slides\_lec6.pdf},
    keywords = {gradient\_based\_descenting, loss\_functions, optimization},
    posted-at = {2015-11-09 16:27:58},
    priority = {2},
    title = {{RMSprop Gradient Optimization}},
    url = {http://www.cs.toronto.edu/\~{}tijmen/csc321/slides/lecture\_slides\_lec6.pdf}
}

@article{lasso,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 volume = {58},
 year = {1996}
}

@article{ridge,
author = { Arthur E.   Hoerl  and  Robert W.   Kennard },
title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
journal = {Technometrics},
volume = {12},
number = {1},
pages = {55-67},
year  = {1970},
publisher = {Taylor & Francis},
doi = {10.1080/00401706.1970.10488634},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/00401706.1970.10488634
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/00401706.1970.10488634
    
}

}

@ARTICLE{batch-norm,
   author = {{Ioffe}, S. and {Szegedy}, C.},
    title = "{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1502.03167},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2015,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150203167I},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{lecun-cnn,
    author = {Yann Lecun and Léon Bottou and Yoshua Bengio and Patrick Haffner},
    title = {Gradient-based learning applied to document recognition},
    booktitle = {Proceedings of the IEEE},
    year = {1998},
    pages = {2278--2324}
}
@inproceedings{krizhevsky-cnn,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
 series = {NIPS'12},
 year = {2012},
 location = {Lake Tahoe, Nevada},
 pages = {1097--1105},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
 acmid = {2999257},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@inproceedings{policy-gradient-sutton,
 author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
 title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
 booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
 series = {NIPS'99},
 year = {1999},
 location = {Denver, CO},
 pages = {1057--1063},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3009657.3009806},
 acmid = {3009806},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@Article{reinforce,
author="Williams, Ronald J.",
title="Simple statistical gradient-following algorithms for connectionist reinforcement learning",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="229--256",
abstract="This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.",
issn="1573-0565",
doi="10.1007/BF00992696",
url="https://doi.org/10.1007/BF00992696"
}

@article{deep-blue,
 author = {Campbell, Murray and Hoane,Jr., A. Joseph and Hsu, Feng-hsiung},
 title = {Deep Blue},
 journal = {Artif. Intell.},
 issue_date = {January 2002},
 volume = {134},
 number = {1-2},
 month = jan,
 year = {2002},
 issn = {0004-3702},
 pages = {57--83},
 numpages = {27},
 url = {http://dx.doi.org/10.1016/S0004-3702(01)00129-1},
 doi = {10.1016/S0004-3702(01)00129-1},
 acmid = {512152},
 publisher = {Elsevier Science Publishers Ltd.},
 address = {Essex, UK},
 keywords = {computer chess, evaluation function, game tree search, parallel search, search extensions, selective search},
} 

@article{policy-iteration,
author="Bertsekas, Dimitri P.",
title="Approximate policy iteration: a survey and some new methods",
journal="Journal of Control Theory and Applications",
year="2011",
month="Aug",
day="01",
volume="9",
number="3",
pages="310--335",
abstract="We consider the classical policy iteration method of dynamic programming (DP), where approximations and simulation are used to deal with the curse of dimensionality. We survey a number of issues: convergence and rate of convergence of approximate policy evaluation methods, singularity and susceptibility to simulation noise of policy evaluation, exploration issues, constrained and enhanced policy iteration, policy oscillation and chattering, and optimistic and distributed policy iteration. Our discussion of policy evaluation is couched in general terms and aims to unify the available methods in the light of recent research developments and to compare the two main policy evaluation approaches: projected equations and temporal differences (TD), and aggregation. In the context of these approaches, we survey two different types of simulation-based algorithms: matrix inversion methods, such as least-squares temporal difference (LSTD), and iterative methods, such as least-squares policy evaluation (LSPE) and TD ($\lambda$), and their scaled variants. We discuss a recent method, based on regression and regularization, which rectifies the unreliability of LSTD for nearly singular projected Bellman equations. An iterative version of this method belongs to the LSPE class of methods and provides the connecting link between LSTD and LSPE. Our discussion of policy improvement focuses on the role of policy oscillation and its effect on performance guarantees. We illustrate that policy evaluation when done by the projected equation/TD approach may lead to policy oscillation, but when done by aggregation it does not. This implies better error bounds and more regular performance for aggregation, at the expense of some loss of generality in cost function representation capability. Hard aggregation provides the connecting link between projected equation/TD-based and aggregation-based policy evaluation, and is characterized by favorable error bounds.",
issn="1993-0623",
doi="10.1007/s11768-011-1005-3",
url="https://doi.org/10.1007/s11768-011-1005-3"
}


